{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferential Statistics Vs. Predictive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- Describe the hallmarks of inferential statistics, and to contrast them with the hallmarks of predictive statistics;\n",
    "- Relate the goals of model-building to expected value, bias and variance\n",
    "- Define error as a function of prediction error and irreducible error\n",
    "- Define prediction error as a combination of bias and variance\n",
    "\n",
    "## Inferential Statistics in a Nutshell\n",
    "\n",
    "In Phase 1 we looked at *descriptive* statistics: starting with a dataset and making various observations (overall shape, histogram, outliers, etc.) as well as calculations of quantities that can characterize the dataset as a whole (mean, median, mode, variance, standard deviation, quartiles, percentiles, etc.).\n",
    "\n",
    "At the beginning of Phase 2 we moved into inferential statistics. The main idea here is to imagine that *we don't have* (or anyway cannot *measure*) all the data of interest.\n",
    "\n",
    "And this is, of course, the typical situation. Consider:\n",
    "\n",
    "- A zoologist wanting to know the typical lifespan of a Siberian tiger\n",
    "- A cosmologist wanting to know the mass of a normal white dwarf star\n",
    "- A businesswoman wanting to know how many M&M's her customers should expect to find in their Party Size bags\n",
    "- A botanist wanting to know how tall California redwoods usually grow\n",
    "\n",
    "The zoologist could, in principle:\n",
    "\n",
    "1. keep track of every currently existing Siberian tiger;\n",
    "2. record their (more or less) exact ages at their moments of death;\n",
    "3. add up those ages and divide by the number of tigers to calculate an average lifespan\n",
    "\n",
    "––But **only** in principle. In all of these situations, there is no realistic or practical opportunity to check each relevant data point.\n",
    "\n",
    "What we can do, however, is to check *some* of the data points we want to check. That is, we'll draw a *sample* of data from our *population* of interest. We can then use the techniques of descriptive statistics to characterize our sample.\n",
    "\n",
    "The hope, then, is that our sample will be *representative* of the population as a whole, which would justify our using facts about the sample to ***infer*** things about the population as a whole. Naturally we'll expect a certain amount of **error**: If I take the mean of a sample, $\\bar{x}$ and project it as an estimate of the mean of the whole population, $\\mu$, the estimate is bound to be imperfect. Etc. etc.\n",
    "\n",
    "Inferential statistics makes all this precise. And that has been the bulk of the content of Phase 2.\n",
    "\n",
    "Classically speaking, inference is a form of learning or of *increasing our knowledge*. So when conducting exercises in inferential statistics, the goal is ultimately **understanding**. If I am conducting a linear regression in an inferential mode, then:\n",
    "\n",
    "- I will be very interested in the values of the coefficients, since these represent the effect of the associated factors on the target in question;\n",
    "- the more data I use to build the regression the better;\n",
    "- the fewer transformations of my data the better, since lots of transformations will impede transparency and comprehensibility;\n",
    "- fewer predictors may be better than more;\n",
    "- I will be very interested in respecting the assumptions of linear regression;\n",
    "- I'll probably choose `statsmodels` if working in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Statistics in a Nutshell\n",
    "\n",
    "The focus for predictive statistics is a bit different.\n",
    "\n",
    "First, the goal is less on understanding and more (of course!) on making good *predictions* of future cases.\n",
    "\n",
    "That means that I want the patterns I pick up on (in some dataset) to be patterns that will *recur* (in a similar dataset) in the future.\n",
    "\n",
    "Needless to say, researchers may care *both* about understanding a process and about predicting the future. Science seems to be involve both. If I am performing a linear regression on data about cigarette smoking paired with data about lifespans, then I may want both to understand exactly how smoking affects lifespan and to make predictions for the future about how long smokers can expect to live.\n",
    "\n",
    "Nevertheless, the difference of emphasis can make for a difference in practice. If I am conducting a linear regression in a predictive mode, then:\n",
    "\n",
    "- I won't particularly care about the values of the coefficients;\n",
    "- I may want to have two different datasets: one on which to build (\"train\") the regression and another on which to **evaluate** (\"test\") the regression. (*Did* the patterns that I picked up on in the first dataset recur in the second?)\n",
    "- I won't particularly care about whether or how the data has been modified or transformed before subjecting it to regression analysis;\n",
    "- more predictors are probably better than fewer;\n",
    "- I won't care as much about respecting the assumptions of linear regression\n",
    "- I'll probably choose `sklearn` if working in Python, since predictive statistics is at the heart of machine learning.\n",
    "\n",
    "Of course, to the extent that we give up on actually trying to *understand* the phenomenon that we are modeling, to that extent we are happy to let our models be **black boxes**. As we move deeper into the course and our models get ever more sophisticated, they will also become ever more like black boxes, for better or for worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `[022221FT]` Topic 20 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling Theory\n",
    "\n",
    "![which model is better](img/which_model_is_better.png)\n",
    "\n",
    "[Netflix example](https://towardsdatascience.com/cultural-overfitting-and-underfitting-or-why-the-netflix-culture-wont-work-in-your-company-af2a62e41288)\n",
    "\n",
    "### What is a “Model”?\n",
    "\n",
    " - A “model” is a general specification of relationships among variables. \n",
    "     - e.g. a linear regression, such as: $ Price = \\beta_1*Time +  \\beta_0 (+ \\epsilon)$\n",
    "\n",
    " - A “trained model” is a particular model that has been built using some training data.\n",
    " a. If the model is **parametric** (like a linear regression), then it has parameters that have been calculated using the training data;\n",
    " b. If the model is **non-parametric**, then it has (not parameters but) an algorithm that has been constructed using the training data.\n",
    "\n",
    "### What makes a model good?\n",
    "\n",
    "- We don’t ultimately care about how well your model fits your data.\n",
    "\n",
    "- What we really care about is how well your model describes the process that generated your data.\n",
    "\n",
    "- Why? Because the data set you have is but one sample from a universe of possible data sets, and you want a model that would work for any data set from that universe.\n",
    "\n",
    "\n",
    "\n",
    "## Return to Expected Value\n",
    "\n",
    "- The expected value of a quantity is the weighted average of that quantity across all possible samples\n",
    "\n",
    "![6 sided die](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/6sided_dice.jpg/600px-6sided_dice.jpg)\n",
    "\n",
    "- for a 6 sided die, another way to think about the expected value is the arithmetic mean of the rolls of a very large number of independent samples.  \n",
    "\n",
    "### The expected value of a 6-sided die is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = 1/6\n",
    "rolls = range(1, 7)\n",
    "\n",
    "expected_value = sum([probs * roll for roll in rolls])\n",
    "expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model bias and variance\n",
    "\n",
    "- Let's imagine we create a model that always predicts a roll of **3**.\n",
    "\n",
    "- **The *bias* is the difference between the average prediction of our model and the average roll of the die as we roll more and more times**.\n",
    "    - What is the bias of a model that always predicts 3?\n",
    "\n",
    "<details>\n",
    "<summary> Answer below</summary>\n",
    "0.5\n",
    "</details><br>\n",
    "\n",
    "- **The *variance* is the average difference between each individual prediction and the average prediction of our model as we roll more and more times**.\n",
    "    - What is the variance of that model?\n",
    "<details>\n",
    "<summary> Answer below\n",
    "</summary>\n",
    "0\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Error: prediction error and irreducible error\n",
    "\n",
    "\n",
    "\n",
    "### Regression fit statistics are often called “error”\n",
    " - Sum of Squared Errors (SSE)\n",
    " $ {\\displaystyle \\operatorname {SSE} =\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.} $\n",
    " - Mean Squared Error (MSE) \n",
    " \n",
    " $ {\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.} $\n",
    " \n",
    " - Root Mean Squared Error (RMSE)  \n",
    " $ {\\displaystyle \\operatorname \n",
    "  {RMSE} =\\sqrt{MSE}} $\n",
    "\n",
    " All are calculated using residuals    \n",
    "\n",
    "![residuals](img/residuals.png)\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    " - Fit a quick and dirty linear regression model\n",
    " - Store predictions in the y_hat variable using predict() from the fit model\n",
    " - Handcode SSE\n",
    " - Divide by the length of array to find Mean Squared Error\n",
    " - Check that your MSE equals sklearn's mean_squared_error function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/king_county.csv', index_col='id')\n",
    "df = df.iloc[:, :12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7129300520</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6414100192</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631500400</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487200875</th>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954400510</th>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "id                                                                         \n",
       "7129300520  221900.0         3       1.00         1180      5650     1.0   \n",
       "6414100192  538000.0         3       2.25         2570      7242     2.0   \n",
       "5631500400  180000.0         2       1.00          770     10000     1.0   \n",
       "2487200875  604000.0         4       3.00         1960      5000     1.0   \n",
       "1954400510  510000.0         3       2.00         1680      8080     1.0   \n",
       "\n",
       "            waterfront  view  condition  grade  sqft_above  sqft_basement  \n",
       "id                                                                         \n",
       "7129300520           0     0          3      7        1180              0  \n",
       "6414100192           0     0          3      7        2170            400  \n",
       "5631500400           0     0          3      6         770              0  \n",
       "2487200875           0     0          5      7        1050            910  \n",
       "1954400510           0     0          3      8        1680              0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = df.price\n",
    "\n",
    "# Build the regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate error\n",
    "# y_hat = None\n",
    "# sse = None\n",
    "# mse = None\n",
    "# rmse = None\n",
    "\n",
    "# # Compare with sklearn\n",
    "# print(rmse)\n",
    "# print(np.sqrt(mean_squared_error(y, y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230587.3189849992\n",
      "230587.31898499973\n"
     ]
    }
   ],
   "source": [
    "# Calculate error\n",
    "y_hat = lr.predict(X)\n",
    "sse = sum((y-y_hat)**2)\n",
    "mse = sse/len(y)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Compare with sklearn\n",
    "print(rmse)\n",
    "print(np.sqrt(mean_squared_error(y, y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining prediction error as a combination of bias and variance\n",
    "\n",
    "$\\Large Total\\ Error\\ = Prediction\\ Error+ Irreducible\\ Error$\n",
    "\n",
    "Our prediction error can be further broken down into error due to bias and error due to variance.\n",
    "\n",
    "$\\Large Total\\ Error = Model\\ Bias^2 + Model\\ Variance + Irreducible\\ Error$\n",
    "\n",
    "**Model Bias** is the expected prediction error of the expected trained model.\n",
    "\n",
    "> In other words, if you were to train multiple models on different samples, what would be the average difference between the prediction and the real value?\n",
    "\n",
    "**Model Variance** is the expected variation in predictions, relative to your expected trained model.\n",
    "\n",
    "> In other words, what would be the average difference between any one model's prediction and the average of all the predictions?\n",
    "\n",
    "**Bias vs. variance refers ultimately to the *accuracy* vs. *consistency* of the models trained by your algorithm.**\n",
    "\n",
    "![target_bias_variance](img/target.png)\n",
    "\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [022221FT] Calculating Bias/Variance -JMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically\n",
    "- https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/\n",
    "- https://rasbt.github.io/mlxtend/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\text{Bias}(\\hat{f}(x)) = E[\\hat{f}(x)-f(x)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/king_county.csv', index_col='id')\n",
    "df = df.iloc[:, :12]\n",
    "\n",
    "X = df.drop('price',axis=1)\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6015578450258983"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6161513065523546"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # !pip install mlxtend\n",
    "# from mlxtend.evaluate import bias_variance_decomp \n",
    "# bias_variance_decomp(lr, X_train.values,y_train.values,X_test.values,y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming up next\n",
    "\n",
    "It goes without saying that we would generally like our models to have both low bias and low variance. But what is not so obvious is that, unfortunately, as one tends to go down, the other tends to go up. Moreover, we shall often be able to tweak model **hyperparameters** with the purpose of decreasing the bias (even if that also means increasing the variance) or of decreasing the variance (even if that also means decreasing the bias). And so we shall soon come to appreciate the ***bias-variance tradeoff*** as it applies to machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
